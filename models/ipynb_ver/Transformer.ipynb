{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM26be2Fre6ruvc4erNG3Xo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"50e5a76c2664410298d09afe0f606592":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_69089560741e450a9a953e0193081f6b","IPY_MODEL_bab3068e0e9e4808b385d5077afda29c","IPY_MODEL_6476ca11c76947cb86eceb0a83d411f5"],"layout":"IPY_MODEL_a07f201b2b0547028ea967eb54056cc0"}},"69089560741e450a9a953e0193081f6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6322cf51827149eaa48999f5ee386b4b","placeholder":"​","style":"IPY_MODEL_8d1b744a43ca4b3a9f3aa63ac8741146","value":"tokenizer_config.json: 100%"}},"bab3068e0e9e4808b385d5077afda29c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_227116f2d96f429284649dcf6937af72","max":44,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42f57e9c5303457fae27bd04e54a0295","value":44}},"6476ca11c76947cb86eceb0a83d411f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bfa685b3a604987960d66d92a6634f4","placeholder":"​","style":"IPY_MODEL_494cc99717644ae5ab5b43f375fb1aca","value":" 44.0/44.0 [00:00&lt;00:00, 1.86kB/s]"}},"a07f201b2b0547028ea967eb54056cc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6322cf51827149eaa48999f5ee386b4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d1b744a43ca4b3a9f3aa63ac8741146":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"227116f2d96f429284649dcf6937af72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42f57e9c5303457fae27bd04e54a0295":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9bfa685b3a604987960d66d92a6634f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"494cc99717644ae5ab5b43f375fb1aca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bba70deaa84140fb979216aade7a9134":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30866497eccf4d4fb90ca33f0781c2b2","IPY_MODEL_290990d0668d46f2be239833f072bd54","IPY_MODEL_7659eeb14035446fb487beb42128eb5a"],"layout":"IPY_MODEL_4f2914851db1471591162ea48a34b878"}},"30866497eccf4d4fb90ca33f0781c2b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcd4d2c4dcd84782bc5a2a4e2049ff27","placeholder":"​","style":"IPY_MODEL_7f52ee2b54f24440b2b1a92bf13623fd","value":"source.spm: 100%"}},"290990d0668d46f2be239833f072bd54":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73c34acf35944930b8c313eb8ccb5ee6","max":841805,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b80fe570936415fa605428f4c203a14","value":841805}},"7659eeb14035446fb487beb42128eb5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e12b4d5a3fb0414c8f55c296fec6d77b","placeholder":"​","style":"IPY_MODEL_f7fd556d5b074bb8b9898c0392a4f1a7","value":" 842k/842k [00:00&lt;00:00, 4.19MB/s]"}},"4f2914851db1471591162ea48a34b878":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcd4d2c4dcd84782bc5a2a4e2049ff27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f52ee2b54f24440b2b1a92bf13623fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73c34acf35944930b8c313eb8ccb5ee6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b80fe570936415fa605428f4c203a14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e12b4d5a3fb0414c8f55c296fec6d77b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7fd556d5b074bb8b9898c0392a4f1a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5b591ae1e014f42a5539552d7fb3551":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e0d17b7ca6c4fe2954f401dee582cfc","IPY_MODEL_99a0bc870e1e42b2b053492b964e7035","IPY_MODEL_91a1b749cc0747bdafd85f72ef5d4429"],"layout":"IPY_MODEL_e0ce839952d54300a6ed2394310fe1ad"}},"5e0d17b7ca6c4fe2954f401dee582cfc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f15c79feb32a4cc48a0a12e4eb3c799e","placeholder":"​","style":"IPY_MODEL_0457294a70524afdab7b0ae68b545f36","value":"target.spm: 100%"}},"99a0bc870e1e42b2b053492b964e7035":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a4d13b8466e4a599aceb3c2671dabce","max":813126,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b764b4e2dfb44f1bbcb58f69b6f77454","value":813126}},"91a1b749cc0747bdafd85f72ef5d4429":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f11b182c24944420a17dc5956fdfa535","placeholder":"​","style":"IPY_MODEL_4049668ec01145d8b87229c4d7c1729f","value":" 813k/813k [00:00&lt;00:00, 7.50MB/s]"}},"e0ce839952d54300a6ed2394310fe1ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f15c79feb32a4cc48a0a12e4eb3c799e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0457294a70524afdab7b0ae68b545f36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a4d13b8466e4a599aceb3c2671dabce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b764b4e2dfb44f1bbcb58f69b6f77454":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f11b182c24944420a17dc5956fdfa535":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4049668ec01145d8b87229c4d7c1729f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4500102253d8490e87d0e929bc3d8bf6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8005718360ca407ea41f926a27a6160c","IPY_MODEL_34c16e817b2345d7a7ad1c16d40f7a78","IPY_MODEL_6012c37178ed4951aa27a8d6374a6753"],"layout":"IPY_MODEL_3b6ee39245a24ec988b2a55f6c3f1881"}},"8005718360ca407ea41f926a27a6160c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d70ae6aea7140ee986d229124ca9944","placeholder":"​","style":"IPY_MODEL_27fdd9269e934ea2ab616af50199e848","value":"vocab.json: 100%"}},"34c16e817b2345d7a7ad1c16d40f7a78":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ac119a552904553841e197039d3c806","max":1719866,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6183b3334b141a385b7f20cbc467205","value":1719866}},"6012c37178ed4951aa27a8d6374a6753":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4a55fc195764f86a663602307c2f29a","placeholder":"​","style":"IPY_MODEL_2c2f3a60ae6248059e4bdf58ea11fa11","value":" 1.72M/1.72M [00:00&lt;00:00, 17.8MB/s]"}},"3b6ee39245a24ec988b2a55f6c3f1881":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d70ae6aea7140ee986d229124ca9944":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27fdd9269e934ea2ab616af50199e848":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ac119a552904553841e197039d3c806":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6183b3334b141a385b7f20cbc467205":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4a55fc195764f86a663602307c2f29a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c2f3a60ae6248059e4bdf58ea11fa11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a4927970c414dd8a6a7e4dcf71d7ee5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21a0a99a693b4bcab820af7f3ab61095","IPY_MODEL_c82a76ca82964420a4a5aaf337c200d9","IPY_MODEL_a53ac2482a1d4b94a0d8ffc0007d4112"],"layout":"IPY_MODEL_5258051b4d4a49db92067f556160afb5"}},"21a0a99a693b4bcab820af7f3ab61095":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_395d686078e74f29b872347b4e69caaf","placeholder":"​","style":"IPY_MODEL_b67a478cc35c4bdaa27d83037a023a40","value":"config.json: 100%"}},"c82a76ca82964420a4a5aaf337c200d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_30432b660ae5499385515db2a7f37b64","max":1394,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec4c4bffd32b494f8f1c3eae46737f71","value":1394}},"a53ac2482a1d4b94a0d8ffc0007d4112":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_879b04407bad4724ab26188750768f0e","placeholder":"​","style":"IPY_MODEL_181d425e75414267a0156573990a54e3","value":" 1.39k/1.39k [00:00&lt;00:00, 38.4kB/s]"}},"5258051b4d4a49db92067f556160afb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"395d686078e74f29b872347b4e69caaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b67a478cc35c4bdaa27d83037a023a40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30432b660ae5499385515db2a7f37b64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec4c4bffd32b494f8f1c3eae46737f71":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"879b04407bad4724ab26188750768f0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"181d425e75414267a0156573990a54e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0662ff4a130d4ee7b6908c8d354d35c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10df715bf9b34d839f3501c594c0dd34","IPY_MODEL_4da85b19f5774b4c94ccc56c004ecfa9","IPY_MODEL_0d4a72a4f5174000a5adcfa10607c501"],"layout":"IPY_MODEL_651a2bf40907430ca1dea1f139926c21"}},"10df715bf9b34d839f3501c594c0dd34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef90baa1d83d48998923cf24ebbb8a3f","placeholder":"​","style":"IPY_MODEL_63f4541bff01468c95baafdbba362dbc","value":"pytorch_model.bin: 100%"}},"4da85b19f5774b4c94ccc56c004ecfa9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d9351441e734435ad2bfc066d246716","max":312087009,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb45cd9d95f04b528806f7309a093f12","value":312087009}},"0d4a72a4f5174000a5adcfa10607c501":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c1a2bf238d94194942e9ca621333940","placeholder":"​","style":"IPY_MODEL_27b2803905c940eca753f03334511024","value":" 312M/312M [00:11&lt;00:00, 26.4MB/s]"}},"651a2bf40907430ca1dea1f139926c21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef90baa1d83d48998923cf24ebbb8a3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63f4541bff01468c95baafdbba362dbc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d9351441e734435ad2bfc066d246716":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb45cd9d95f04b528806f7309a093f12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c1a2bf238d94194942e9ca621333940":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27b2803905c940eca753f03334511024":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90c34a1ffc15404fab8f3051ffca6230":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e4ab3a761a84239987809b372a992ad","IPY_MODEL_d14d00fbf0b34ab0b57fca05f3dcf264","IPY_MODEL_83df45e512834361a400c2fc24a8bbe4"],"layout":"IPY_MODEL_7a80c496931542ef9f396bc52293f96c"}},"3e4ab3a761a84239987809b372a992ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26ea1c41da0b4a5090ebf0fc63a815da","placeholder":"​","style":"IPY_MODEL_73c4ad40a3ed4fba8fca2bbd0134d048","value":"generation_config.json: 100%"}},"d14d00fbf0b34ab0b57fca05f3dcf264":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1281e1f15184539b761dcf8da3610be","max":293,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6728a93659c544709cdd8d671d1e7c18","value":293}},"83df45e512834361a400c2fc24a8bbe4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fb14ba9a1294520815a792788f066b2","placeholder":"​","style":"IPY_MODEL_26a2de2052bc4ec48f69be8dd3d78836","value":" 293/293 [00:00&lt;00:00, 7.88kB/s]"}},"7a80c496931542ef9f396bc52293f96c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26ea1c41da0b4a5090ebf0fc63a815da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73c4ad40a3ed4fba8fca2bbd0134d048":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1281e1f15184539b761dcf8da3610be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6728a93659c544709cdd8d671d1e7c18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fb14ba9a1294520815a792788f066b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26a2de2052bc4ec48f69be8dd3d78836":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"T5OUPvcQoAUb","executionInfo":{"status":"ok","timestamp":1701757354676,"user_tz":-540,"elapsed":4561,"user":{"displayName":"SeongWoo Jeong","userId":"08005906526700565399"}}},"outputs":[],"source":["%%capture\n","# capture는 주렁주렁 출력 보여주지 않게끔\n","# 그래프에서 한글이 깨지지 않게 폰트 설치..\n","# *맨처음 실행 후 런타임 다시 시작해야 반영됨!!\n","!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf\n","import matplotlib.pyplot as plt\n","plt.rc('font', family='NanumBarunGothic')"]},{"cell_type":"code","source":["%%capture\n","!pip install gdown\n","!pip install transformers # hugging face lib\n","!pip install sentencepiece # MarianTokenizer 불러올 때 필요\n","!pip install sacremoses # MarianMTModel 에서 불러올 때 warning 뜨는 것 방지\n","!pip install einops # 지리는 einops 쓰기 아인슈타인 오퍼레이션(eniops)"],"metadata":{"id":"bYvcw0y0oDbo","executionInfo":{"status":"ok","timestamp":1701757392386,"user_tz":-540,"elapsed":37719,"user":{"displayName":"SeongWoo Jeong","userId":"08005906526700565399"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import torch\n","from torch import nn, optim\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from transformers import MarianMTModel, MarianTokenizer\n","import pandas as pd\n","from tqdm import tqdm\n","import math\n","from einops import rearrange\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(DEVICE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPbXIQTUoDee","executionInfo":{"status":"ok","timestamp":1701757421279,"user_tz":-540,"elapsed":28911,"user":{"displayName":"SeongWoo Jeong","userId":"08005906526700565399"}},"outputId":"ee1da68c-ce67-4b12-bc54-7c6ba8cbf2b3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","cuda\n"]}]},{"cell_type":"code","source":["# Load the tokenizer & input embedding layer & last fc layer\n","tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n","model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ko-en') # MT: Machine Translation\n","# input_embedding = model.get_input_embeddings() # 일부 사이즈 큰 놈들은 데려와서 나머지만 학습시키려는 시도\n","# input_embedding.weight.requires_grad = False # freeze\n","# fc_out = model.get_output_embeddings()\n","# fc_out.weight.requires_grad = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["50e5a76c2664410298d09afe0f606592","69089560741e450a9a953e0193081f6b","bab3068e0e9e4808b385d5077afda29c","6476ca11c76947cb86eceb0a83d411f5","a07f201b2b0547028ea967eb54056cc0","6322cf51827149eaa48999f5ee386b4b","8d1b744a43ca4b3a9f3aa63ac8741146","227116f2d96f429284649dcf6937af72","42f57e9c5303457fae27bd04e54a0295","9bfa685b3a604987960d66d92a6634f4","494cc99717644ae5ab5b43f375fb1aca","bba70deaa84140fb979216aade7a9134","30866497eccf4d4fb90ca33f0781c2b2","290990d0668d46f2be239833f072bd54","7659eeb14035446fb487beb42128eb5a","4f2914851db1471591162ea48a34b878","dcd4d2c4dcd84782bc5a2a4e2049ff27","7f52ee2b54f24440b2b1a92bf13623fd","73c34acf35944930b8c313eb8ccb5ee6","5b80fe570936415fa605428f4c203a14","e12b4d5a3fb0414c8f55c296fec6d77b","f7fd556d5b074bb8b9898c0392a4f1a7","c5b591ae1e014f42a5539552d7fb3551","5e0d17b7ca6c4fe2954f401dee582cfc","99a0bc870e1e42b2b053492b964e7035","91a1b749cc0747bdafd85f72ef5d4429","e0ce839952d54300a6ed2394310fe1ad","f15c79feb32a4cc48a0a12e4eb3c799e","0457294a70524afdab7b0ae68b545f36","4a4d13b8466e4a599aceb3c2671dabce","b764b4e2dfb44f1bbcb58f69b6f77454","f11b182c24944420a17dc5956fdfa535","4049668ec01145d8b87229c4d7c1729f","4500102253d8490e87d0e929bc3d8bf6","8005718360ca407ea41f926a27a6160c","34c16e817b2345d7a7ad1c16d40f7a78","6012c37178ed4951aa27a8d6374a6753","3b6ee39245a24ec988b2a55f6c3f1881","5d70ae6aea7140ee986d229124ca9944","27fdd9269e934ea2ab616af50199e848","1ac119a552904553841e197039d3c806","e6183b3334b141a385b7f20cbc467205","e4a55fc195764f86a663602307c2f29a","2c2f3a60ae6248059e4bdf58ea11fa11","3a4927970c414dd8a6a7e4dcf71d7ee5","21a0a99a693b4bcab820af7f3ab61095","c82a76ca82964420a4a5aaf337c200d9","a53ac2482a1d4b94a0d8ffc0007d4112","5258051b4d4a49db92067f556160afb5","395d686078e74f29b872347b4e69caaf","b67a478cc35c4bdaa27d83037a023a40","30432b660ae5499385515db2a7f37b64","ec4c4bffd32b494f8f1c3eae46737f71","879b04407bad4724ab26188750768f0e","181d425e75414267a0156573990a54e3","0662ff4a130d4ee7b6908c8d354d35c9","10df715bf9b34d839f3501c594c0dd34","4da85b19f5774b4c94ccc56c004ecfa9","0d4a72a4f5174000a5adcfa10607c501","651a2bf40907430ca1dea1f139926c21","ef90baa1d83d48998923cf24ebbb8a3f","63f4541bff01468c95baafdbba362dbc","7d9351441e734435ad2bfc066d246716","bb45cd9d95f04b528806f7309a093f12","2c1a2bf238d94194942e9ca621333940","27b2803905c940eca753f03334511024","90c34a1ffc15404fab8f3051ffca6230","3e4ab3a761a84239987809b372a992ad","d14d00fbf0b34ab0b57fca05f3dcf264","83df45e512834361a400c2fc24a8bbe4","7a80c496931542ef9f396bc52293f96c","26ea1c41da0b4a5090ebf0fc63a815da","73c4ad40a3ed4fba8fca2bbd0134d048","c1281e1f15184539b761dcf8da3610be","6728a93659c544709cdd8d671d1e7c18","0fb14ba9a1294520815a792788f066b2","26a2de2052bc4ec48f69be8dd3d78836"]},"id":"_qyJ0HYDoDhZ","executionInfo":{"status":"ok","timestamp":1701757736038,"user_tz":-540,"elapsed":26579,"user":{"displayName":"SeongWoo Jeong","userId":"08005906526700565399"}},"outputId":"152b7c69-4d90-4125-ed77-4f80e8da0fbe"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50e5a76c2664410298d09afe0f606592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["source.spm:   0%|          | 0.00/842k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba70deaa84140fb979216aade7a9134"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["target.spm:   0%|          | 0.00/813k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5b591ae1e014f42a5539552d7fb3551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4500102253d8490e87d0e929bc3d8bf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a4927970c414dd8a6a7e4dcf71d7ee5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0662ff4a130d4ee7b6908c8d354d35c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c34a1ffc15404fab8f3051ffca6230"}},"metadata":{}}]},{"cell_type":"code","source":["eos_idx = tokenizer.eos_token_id\n","pad_idx = tokenizer.pad_token_id\n","print(\"eos_idx = \", eos_idx)\n","print(\"pad_idx = \", pad_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfYTQCK7oDmJ","executionInfo":{"status":"ok","timestamp":1701757760213,"user_tz":-540,"elapsed":4,"user":{"displayName":"SeongWoo Jeong","userId":"08005906526700565399"}},"outputId":"5fb6ffef-cd7d-40f9-9f95-3595dbf8bd6b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["eos_idx =  0\n","pad_idx =  65000\n"]}]},{"cell_type":"markdown","source":["### 하이퍼 파라미터 조정"],"metadata":{"id":"aFVHrtDnqtex"}},{"cell_type":"code","source":["BATCH_SIZE = 64 # 논문에선 2.5만 token이 한 batch에 담기게 했다고 함.\n","LAMBDA = 0 # l2-Regularization를 위한 hyperparam. # 저장된 모델\n","EPOCH = 15 # 저장된 모델\n","# max_len = 512 # model.model.encoder.embed_positions 를 보면 512로 했음을 알 수 있다.\n","max_len = 100 # 너무 긴거 같아서 자름 (GPU 부담도 많이 덜어짐)\n","criterion = nn.CrossEntropyLoss(ignore_index = pad_idx) # pad token 이 출력 나와야하는 시점의 loss는 무시 (즉, label이 <pad> 일 때는 무시) # 저장된 모델\n","# criterion = nn.CrossEntropyLoss(ignore_index = pad_idx, label_smoothing = 0.1) # 막생 해보니 성능 안나옴 <- 데이터가 많아야 할 듯\n","\n","scheduler_name = 'Noam'\n","# scheduler_name = 'Cos'\n","#### Noam ####\n","# warmup_steps = 4000 # 이건 논문에서 제시한 값 (총 10만 step의 4%)\n","warmup_steps = 1500 # 데이터 수 * EPOCH / BS = 총 step 수 인것 고려 # 저장된 모델\n","LR_scale = 0.5 # Noam scheduler에 peak LR 값 조절을 위해 곱해질 녀석 # 저장된 모델\n","#### Cos ####\n","LR_init = 5e-4\n","T0 = 1500 # 첫 주기\n","T_mult = 2 # 배 만큼 주기가 길어짐 (1보다 큰 정수여야 함)\n","#############\n","\n","new_model_train = False\n","hyuk_model_use = True # 여러분만의 모델 만들어서 사용하고 싶다면 False로\n","if hyuk_model_use:\n","    !gdown https://drive.google.com/uc?id=16YgP0smjPPVhW7gm2BdvTfbRays2umYs -O Transformer_small.pt\n","    !gdown https://drive.google.com/uc?id=1-4nc6N2d3DR_LBDfGOtTHBptFIXAiFbO -O Transformer_small_history.pt\n","    save_model_path = 'Transformer_small.pt'\n","    save_history_path = 'Transformer_small_history.pt'\n","else:\n","    save_model_path = '/content/drive/MyDrive/Colab Notebooks/results/Transformer_small2.pt'\n","    save_history_path = '/content/drive/MyDrive/Colab Notebooks/results/Transformer_small2_history.pt'"],"metadata":{"id":"vBE_9XOioDpG","executionInfo":{"status":"ok","timestamp":1701757421280,"user_tz":-540,"elapsed":7,"user":{"displayName":"SeongWoo Jeong","userId":"08005906526700565399"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["vocab_size = tokenizer.vocab_size\n","print(vocab_size)\n","\n","# 논문에 나오는 base 모델 (train loss를 많이 줄이려면 많은 Epoch이 요구됨, 또, test 성능도 좋으려면 더 많은 데이터 요구)\n","# n_layers = 6\n","# d_model = 512\n","# d_ff = 2048\n","# n_heads = 8\n","# drop_p = 0.1\n","\n","# 좀 사이즈 줄인 모델 (훈련된 input_embedding, fc_out 사용하면 사용 불가)\n","n_layers = 3\n","d_model = 256\n","d_ff = 512\n","n_heads = 8\n","drop_p = 0.1"],"metadata":{"id":"DUsjD8I3oDsk","executionInfo":{"status":"ok","timestamp":1701757421281,"user_tz":-540,"elapsed":8,"user":{"displayName":"SeongWoo Jeong","userId":"08005906526700565399"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## 토크나이저 & 학습된 모델 써보기"],"metadata":{"id":"Iewq8YnA6BNP"}},{"cell_type":"code","source":["# tokenizer 써보기 (_로 띄어쓰기를 나타낸다! 즉, _가 없으면 이어진 한 단어임을 나타냄 subword tokenizing)\n","# 토크나이저에 대해 참고 자료: https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/\n","print(tokenizer.tokenize(\"Hi, I'm Hyuk. ...        a   a?\"))\n","print(tokenizer.tokenize(\"a/b 1+2+3 2:1 a>b\"))\n","print(tokenizer.tokenize(\"pretrained restart\"))\n","print(tokenizer.tokenize(\"chatGPT\"))\n","print(tokenizer.tokenize(\"The example is very good in our lecture\")) # 띄어쓰기도 tokenize 할 때가 있다.\n","print(tokenizer.tokenize(\"한글은 어떻게 할까?\"))\n","print(tokenizer.tokenize(\"확실히 띄어쓰기 기준으로 토크나이징을 하는 것 같진 않다.\"))\n","print(tokenizer.tokenize(\"조사는? 나는 너는 우리는 강의는 너와 나의 내가 너가 나무가 헿\"))\n","print(tokenizer.tokenize(\"마음대로 추가 문장\"))"],"metadata":{"id":"4_iC8QszoDvR","executionInfo":{"status":"ok","timestamp":1701757421281,"user_tz":-540,"elapsed":7,"user":{"displayName":"SeongWoo Jeong","userId":"08005906526700565399"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.get_vocab())\n","print(tokenizer.vocab_size)\n","\n","print(tokenizer.encode('지능', add_special_tokens=False)) # string to index\n","print(tokenizer.encode('<pad>', add_special_tokens=False)) # <pad>는 65000\n","print(tokenizer.encode('</s>', add_special_tokens=False)) # <sos> or <eos>는 0\n","print(tokenizer.encode('He', add_special_tokens=False)) # add_special_tokens=False 는 <eos> 자동 붙여주는 것을 방지\n","print(tokenizer.encode('he', add_special_tokens=False)) # 대소문자 다른 단어로 인식\n","print(tokenizer.tokenize('문장을 넣으면 토크나이즈해서 숫자로 바꾼다'))\n","print(tokenizer.encode('문장을 넣으면 토크나이즈해서 숫자로 바꾼다', add_special_tokens=False))\n","print(tokenizer.decode([204]))\n","print(tokenizer.decode([206]))\n","print(tokenizer.decode([210]))\n","print(tokenizer.decode(list(range(15)) + [65000,65001,65002,65003]))"],"metadata":{"id":"Vo6Lxoa076SQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 사전 학습된 모델로 번역해보기 (생각보다 성능 좋네)\n","input_text = \"오늘도 열심히 일을 하네\"\n","\n","input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n","translated_tokens = model.generate(input_tokens, max_new_tokens=max_len)\n","translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n","\n","print(\"입력:\", input_text)\n","print(\"AI의 번역:\", translated_text)"],"metadata":{"id":"SOH1I1nc76Uo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DS, DL 생성 & 테스트"],"metadata":{"id":"cAfxzmVD9AQb"}},{"cell_type":"code","source":["# data 다운\n","%%capture\n","# https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=126 에서 받을 수 있어요\n","!gdown https://drive.google.com/uc?id=14lAjaR2dRp5p5kEsm5GnwNM9KH-VgoOq -O 대화체.xlsx"],"metadata":{"id":"h8hmS_OU76Xv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.data.loc[idx, '원문'], self.data.loc[idx, '번역문']\n","\n","data = pd.read_excel('대화체.xlsx')\n","custom_DS = CustomDataset(data)\n","\n","train_DS, val_DS, test_DS, _ = torch.utils.data.random_split(custom_DS, [95000, 2000, 1000, len(custom_DS)-95000-2000-1000])\n","# 논문에서는 450만개 영,독 문장 pair 사용\n","\n","train_DL = torch.utils.data.DataLoader(train_DS, batch_size=BATCH_SIZE, shuffle=True)\n","val_DL = torch.utils.data.DataLoader(val_DS, batch_size=BATCH_SIZE, shuffle=True)\n","test_DL = torch.utils.data.DataLoader(test_DS, batch_size=BATCH_SIZE, shuffle=True)\n","\n","print(len(train_DS))\n","print(len(val_DS))\n","print(len(test_DS))"],"metadata":{"id":"ucjjAMzo76aX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_DS 테스트\n","i = 5\n","idx = test_DS.indices[i]\n","print(idx) # 엑셀 파일에서 idx번째 문장에 들어있음을 확인할 수 있다\n","src_text, trg_text = custom_DS.__getitem__(idx)\n","print(src_text)\n","print(trg_text)"],"metadata":{"id":"uWYjDTWE9vkT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_DL 테스트\n","src_texts, trg_texts = next(iter(train_DL)) # 데이터 한 국자 뜨기\n","\n","print(src_texts)\n","print(trg_texts)\n","print(len(src_texts))\n","print(len(trg_texts))\n","\n","# 여러 문장에 대해서는 tokenizer.encode() 가 아닌 그냥 tokenizer()\n","src = tokenizer(src_texts, padding=True, truncation=True, max_length = max_len, return_tensors='pt').input_ids # pt: pytorch tensor로 변환\n","# add_special_tokens = True (default)면 마지막 토큰에 <eos> 가 붙어 나옴\n","# truncation = True: max_len 보다 길면 끊고 <eos> 집어넣어버림\n","# src에 <eos>가 있는 게 반드시 좋은 건지는 알 수 없지만 그냥 붙여봤어요..\n","trg_texts = ['</s> ' + s for s in trg_texts]\n","trg = tokenizer(trg_texts, padding=True, truncation=True, max_length = max_len, return_tensors='pt').input_ids\n","\n","print(src[:2])\n","print(trg[:2])\n","print(src.shape)\n","print(trg.shape)\n","print(trg[:,-1]) # 가장 마지막 단어를 보니 어떤 문장은 <eos> 로 끝이 났고 나머지는 <pad> 로 끝이 났다는 걸 볼 수 있음\n","print(tokenizer.decode(trg[trg[:,-1]==eos_idx,:][0])) # 가장 긴 문장 중 첫 번째 문장 관찰\n","print(trg[5,:-1]) # 디코더 입력\n","print(trg[5,1:]) # 디코더 출력\n","# 그런데 [:,:-1] 로 주면 패딩된 문장은 eos도 넣는 셈 아닌가? 맞다! 하지만 괜찮다. 어차피 출력으로 pad token이 기다리고 있으니.. (loss에서 ignore됨)"],"metadata":{"id":"wtjZ5sLv9vqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 내가 쓸 train data 에 대해서 기존 model이 잘 번역하는지 확인\n","i = 5\n","idx = train_DS.indices[i]\n","src_text, trg_text = custom_DS.__getitem__(idx)\n","print(f\"입력: {src_text}\")\n","print(f\"정답: {trg_text}\")\n","\n","src = tokenizer.encode(src_text, return_tensors='pt', add_special_tokens = True)\n","# add_special_tokens = False 해보면 뭔가 이상하게 번역함 (학습 때 source에도 <eos>를 넣었단 증거?)\n","translated_tokens = model.generate(src, max_new_tokens=max_len)\n","translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=False)\n","\n","print(f\"AI의 번역: {translated_text}\") # 디코더 첫 입력으로 <pad> 토큰을 넣었음. (<pad>를 <sos>로 사용)"],"metadata":{"id":"SMYsB3M29vuG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 구현"],"metadata":{"id":"ij61cYW8_dMk"}},{"cell_type":"markdown","source":["### Multi-Head Attention"],"metadata":{"id":"Rx2JxEBI_k5K"}},{"cell_type":"code","source":["class MHA(nn.Module):\n","    def __init__(self, d_model, n_heads):\n","        super().__init__()\n","\n","        self.n_heads = n_heads\n","\n","        self.fc_q = nn.Linear(d_model, d_model) # 차 or 개x차 or 개x개x차 로 입력해줘야\n","        self.fc_k = nn.Linear(d_model, d_model)\n","        self.fc_v = nn.Linear(d_model, d_model)\n","        self.fc_o = nn.Linear(d_model, d_model)\n","\n","        self.scale = torch.sqrt(torch.tensor(d_model / n_heads))\n","\n","    def forward(self, Q, K, V, mask = None):\n","\n","        Q = self.fc_q(Q) # 개단차\n","        K = self.fc_k(K)\n","        V = self.fc_v(V)\n","\n","        Q = rearrange(Q, '개 단 (헤 차) -> 개 헤 단 차', 헤 = self.n_heads) # 개단차 -> 개헤단차\n","        K = rearrange(K, '개 단 (헤 차) -> 개 헤 단 차', 헤 = self.n_heads)\n","        V = rearrange(V, '개 단 (헤 차) -> 개 헤 단 차', 헤 = self.n_heads)\n","\n","        attention_score = Q @ K.transpose(-2,-1)/self.scale # 개헤단단\n","\n","        if mask is not None:\n","            attention_score[mask] = -1e10\n","        attention_weights = torch.softmax(attention_score, dim=-1) # 개헤단단\n","\n","        attention = attention_weights @ V # 개헤단차\n","\n","        x = rearrange(attention, '개 헤 단 차 -> 개 단 (헤 차)') # 개헤단차 -> 개단차\n","        x = self.fc_o(x) # 개단차\n","\n","        return x, attention_weights\n","\n","        # einops를 알기전\n","        # batch_size = Q.shape[0]\n","\n","        # Q = self.fc_q(Q) # 개단차\n","        # K = self.fc_k(K)\n","        # V = self.fc_v(V)\n","\n","        # Q = Q.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3) # 개헤단차를 만들어줘야 한다 (QK^TV 연산을 위해)\n","        # K = K.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3) # 이 때, 개단차 -> 개단헤차 -> 개헤단차\n","        # V = V.reshape(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3) # 이 순서로 해야 reshape할 때 512를 8개로 쪼갬!!!\n","\n","        # attention_score = Q@K.permute(0,1,3,2)/self.scale # 개헤단단\n","\n","        # if mask is not None:\n","        #     attention_score[mask] = -1e10 # mask.shape = 개헤단단\n","        # attention_weights = torch.softmax(attention_score, dim=-1) # 개헤단단\n","\n","        # attention = attention_weights @ V # 개헤단차\n","\n","        # x = attention.permute(0,2,1,3) # 개단헤차\n","        # x = x.reshape(batch_size, -1, self.d_model) # 개단차\n","\n","        # x = self.fc_o(x) # 개단차\n","\n","        # return x, attention_weights\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, drop_p):\n","        super().__init__()\n","\n","        self.linear = nn.Sequential(nn.Linear(d_model, d_ff),\n","                                    nn.ReLU(),\n","                                    nn.Dropout(drop_p), # 논문에는 명시되어 있지 않지만.. overfitting 취약한 부분이라\n","                                    nn.Linear(d_ff, d_model))\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","        return x"],"metadata":{"id":"0-BTiT2K_kPM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### rearrange, scaling 실험"],"metadata":{"id":"pADmJmD6Dyeo"}},{"cell_type":"code","source":["Q = torch.randn(1, 4, 6) # 개단차\n","print(Q)\n","\n","Q = rearrange(Q, '개 단 (헤 차) -> 개 헤 단 차', 헤 = 3) # 개단차 -> 개헤단차\n","# 주의! (차 헤) 로 해보면 또 다름. 차원의 수 6을 헤드의 수 3으로 쪼개는 것이므로 (헤 차) 가 맞다\n","print(Q)\n","\n","x = rearrange(Q, '개 헤 단 차 -> 개 단 (헤 차)') # 개헤단차 -> 개단차,\n","print(x)\n","print(x.shape)"],"metadata":{"id":"pQdNdfP-_kRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q = torch.randn(1, 4, 6) # 개단차\n","# K = torch.randn(1, 4, 6)\n","# V = torch.randn(1, 4, 6)\n","# print(Q)\n","\n","# Q = Q.reshape(1, -1, 3, 2).permute(0,2,1,3) # 개헤단차를 만들어줘야 한다 (QK^TV 연산을 위해)\n","# K = K.reshape(1, -1, 3, 2).permute(0,2,1,3) # 이 때, 개단차 -> 개단헤차 -> 개헤단차\n","# V = V.reshape(1, -1, 3, 2).permute(0,2,1,3) # 이 순서로 해야 reshape할 때 512를 8개로 쪼갬!!!\n","# print(Q)\n","\n","# attention_score = Q@K.permute(0,1,3,2) # 개헤단단\n","\n","# attention_weights = torch.softmax(attention_score, dim=-1) # 개헤단단\n","\n","# attention = attention_weights @ V # 개헤단차\n","# print(attention)\n","\n","# x = attention.permute(0,2,1,3) # 개단헤차\n","# x = x.reshape(1, -1, 6) # 개단차\n","# print(x)"],"metadata":{"id":"LtThoUbf_kUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 왜 scaling을 sqrt(dk) 로? <- 일단 나누는 이유: 안나누면 softmax로 들어갈때 너무 값이 커서 grad가 너무 작아짐\n","n=torch.arange(1,513)\n","N=1000\n","result=[]\n","for ni in n:\n","    inner_prod = torch.zeros(N)\n","    for i in range(N):\n","        inner_prod[i] = torch.randn(ni,1).T@torch.randn(ni,1) # 내적값을 N번 구해서 var 구해보자\n","    result += [torch.var(inner_prod)]\n","\n","plt.plot(n,result)\n","plt.plot(n,n,'r') # variance 가 점점커지니까 std로 나눠서 unit variance로 만들자는 것!"],"metadata":{"id":"h6YuZ8p9_kW9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"z7A39ANKEBcJ"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, d_ff, n_heads, drop_p):\n","        super().__init__()\n","\n","        self.self_atten = MHA(d_model, n_heads)\n","        self.self_atten_LN = nn.LayerNorm(d_model)\n","\n","        self.FF = FeedForward(d_model, d_ff, drop_p)\n","        self.FF_LN = nn.LayerNorm(d_model)\n","\n","        self.dropout = nn.Dropout(drop_p)\n","\n","    def forward(self, x, enc_mask):\n","\n","        # 인코더에도 마스크가?! -> padding 토큰이 들어있을 때 softmax에 값에 영향을 주니깐 padding 토큰에 대해서 mask하자.\n","        residual, atten_enc = self.self_atten(x, x, x, enc_mask)\n","        residual = self.dropout(residual)\n","        x = self.self_atten_LN(x + residual)\n","\n","        residual = self.FF(x)\n","        residual = self.dropout(residual)\n","        x = self.FF_LN(x + residual)\n","\n","        return x, atten_enc\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n","        super().__init__()\n","\n","        self.scale = torch.sqrt(torch.tensor(d_model))\n","        self.input_embedding = input_embedding\n","        self.pos_embedding = nn.Embedding(max_len, d_model)\n","\n","        self.dropout = nn.Dropout(drop_p)\n","\n","        self.layers = nn.ModuleList([EncoderLayer(d_model, d_ff, n_heads, drop_p) for _ in range(n_layers)])\n","\n","    def forward(self, src, mask, atten_map_save = False): # src.shape = 개단, mask.shape = 개헤단단\n","\n","        pos = torch.arange(src.shape[1]).repeat(src.shape[0], 1).to(DEVICE) # 개단\n","\n","        x = self.scale*self.input_embedding(src) + self.pos_embedding(pos) # 개단차\n","        # self.scale 을 곱해주면 position 보다 token 정보를 더 보게 된다 (gradient에 self.scale 만큼이 더 곱해짐)\n","        x = self.dropout(x)\n","\n","        atten_encs = torch.tensor([]).to(DEVICE)\n","        for layer in self.layers:\n","            x, atten_enc = layer(x, mask)\n","            if atten_map_save is True:\n","                atten_encs = torch.cat([atten_encs , atten_enc[0].unsqueeze(0)], dim=0) # 층헤단단 ㅋ\n","\n","        return x, atten_encs"],"metadata":{"id":"iu7CT_I5D39R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### embedding, repeat 실험"],"metadata":{"id":"U8deET9uG3IZ"}},{"cell_type":"code","source":["# repeat에 대해\n","A = torch.rand(2,3)\n","A_repeat=A.repeat(3,3,2)\n","print(A)\n","print(A_repeat)\n","print(A_repeat.shape)\n","\n","print(torch.arange(5).repeat(3, 1)) # 세 개 문장 5개 단어"],"metadata":{"id":"DTUakZ_hD4AK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nn.Embedding 실험\n","emb=nn.Embedding(10,5) # one-hot encoding 된 벡터가 통과된다는 것이 약속된 상태의 FC layer 인 것\n","print(emb.weight.shape) # weight 개수는 nn.Linear(10,5) 과 동일! 동작 방식이 다른 것\n","print(emb(torch.tensor(2)))\n","\n","print(emb(torch.tensor([[6,1,2],[2,1,7]]))) # 2 개 문장, 3개 단어\n","print(emb(torch.tensor([[6,1,2],[2,1,7]])).shape)"],"metadata":{"id":"mGBOaHisD4Cm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Decoder, 모델 전체"],"metadata":{"id":"03SQ0Ew5KXBb"}},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, d_ff, n_heads, drop_p):\n","        super().__init__()\n","\n","        self.self_atten = MHA(d_model, n_heads)\n","        self.self_atten_LN = nn.LayerNorm(d_model)\n","\n","        self.enc_dec_atten = MHA(d_model, n_heads)\n","        self.enc_dec_atten_LN = nn.LayerNorm(d_model)\n","\n","        self.FF = FeedForward(d_model, d_ff, drop_p)\n","        self.FF_LN = nn.LayerNorm(d_model)\n","\n","        self.dropout = nn.Dropout(drop_p)\n","\n","    def forward(self, x, enc_out, dec_mask, enc_dec_mask):\n","\n","        residual, atten_dec = self.self_atten(x, x, x, dec_mask)\n","        residual = self.dropout(residual)\n","        x = self.self_atten_LN(x + residual)\n","\n","        residual, atten_enc_dec = self.enc_dec_atten(x, enc_out, enc_out, enc_dec_mask) # Q는 디코더로부터 K,V는 인코더로부터!!\n","        residual = self.dropout(residual)\n","        x = self.enc_dec_atten_LN(x + residual)\n","\n","        residual = self.FF(x)\n","        residual = self.dropout(residual)\n","        x = self.FF_LN(x + residual)\n","\n","        return x, atten_dec, atten_enc_dec\n","\n","class Decoder(nn.Module):\n","    def __init__(self, input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n","        super().__init__()\n","\n","        self.scale = torch.sqrt(torch.tensor(d_model))\n","        self.input_embedding = input_embedding\n","        self.pos_embedding = nn.Embedding(max_len, d_model)\n","\n","        self.dropout = nn.Dropout(drop_p)\n","\n","        self.layers = nn.ModuleList([DecoderLayer(d_model, d_ff, n_heads, drop_p) for _ in range(n_layers)])\n","\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","        # self.fc_out = fc_out\n","\n","    def forward(self, trg, enc_out, dec_mask, enc_dec_mask, atten_map_save = False): # trg.shape = 개단, enc_out.shape = 개단차, dec_mask.shape = 개헤단단\n","\n","        pos = torch.arange(trg.shape[1]).repeat(trg.shape[0], 1).to(DEVICE) # 개단\n","\n","        x = self.scale*self.input_embedding(trg) + self.pos_embedding(pos) # 개단차\n","        # self.scale 을 곱해주면 position 보다 token 정보를 더 보게 된다 (gradient에 self.scale 만큼이 더 곱해짐)\n","        x = self.dropout(x)\n","\n","        atten_decs = torch.tensor([]).to(DEVICE)\n","        atten_enc_decs = torch.tensor([]).to(DEVICE)\n","        for layer in self.layers:\n","            x, atten_dec, atten_enc_dec = layer(x, enc_out, dec_mask, enc_dec_mask)\n","            if atten_map_save is True:\n","                atten_decs = torch.cat([atten_decs , atten_dec[0].unsqueeze(0)], dim=0) # 층헤단단 ㅋ\n","                atten_enc_decs = torch.cat([atten_enc_decs , atten_enc_dec[0].unsqueeze(0)], dim=0) # 층헤단단 ㅋ\n","\n","        x = self.fc_out(x)\n","\n","        return x, atten_decs, atten_enc_decs"],"metadata":{"id":"m73hM-5vKYRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, vocab_size, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n","        super().__init__()\n","\n","        self.input_embedding = nn.Embedding(vocab_size, d_model)\n","        self.encoder = Encoder(self.input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p)\n","        self.decoder = Decoder(self.input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p)\n","\n","        self.n_heads = n_heads\n","\n","        # for m in self.modules():\n","        #     if hasattr(m,'weight') and m.weight.dim() > 1: # layer norm에 대해선 initial 안하겠다는 뜻\n","        #         nn.init.kaiming_uniform_(m.weight) # Kaiming의 분산은 2/Nin\n","\n","        for m in self.modules():\n","            if hasattr(m,'weight') and m.weight.dim() > 1: # 인풋 임베딩은 그대로 쓰기 위함\n","                nn.init.xavier_uniform_(m.weight) # xavier의 분산은 2/(Nin+Nout) 즉, 분산이 더 작다. => 그래서 sigmoid/tanh에 적합한 것! (vanishing gradient 막기 위해)\n","\n","    def make_enc_mask(self, src): # src.shape = 개단\n","\n","        enc_mask = (src == pad_idx).unsqueeze(1).unsqueeze(2) # 개11단\n","        enc_mask = enc_mask.repeat(1, self.n_heads, src.shape[1], 1) # 개헤단단\n","        \"\"\" src pad mask (문장 마다 다르게 생김. 이건 한 문장에 대한 pad 행렬)\n","        F F T T\n","        F F T T\n","        F F T T\n","        F F T T\n","        \"\"\"\n","        return enc_mask\n","\n","    def make_dec_mask(self, trg): # trg.shape = 개단\n","\n","        trg_pad_mask = (trg.to('cpu') == pad_idx).unsqueeze(1).unsqueeze(2) # 개11단\n","        trg_pad_mask = trg_pad_mask.repeat(1, self.n_heads, trg.shape[1], 1) # 개헤단단\n","        \"\"\" trg pad mask\n","        F F F T T\n","        F F F T T\n","        F F F T T\n","        F F F T T\n","        F F F T T\n","        \"\"\"\n","        trg_future_mask = torch.tril(torch.ones(trg.shape[0], self.n_heads, trg.shape[1], trg.shape[1]))==0 # 개헤단단\n","        \"\"\" trg future mask\n","        F T T T T\n","        F F T T T\n","        F F F T T\n","        F F F F T\n","        F F F F F\n","        \"\"\"\n","        dec_mask = trg_pad_mask | trg_future_mask # dec_mask.shape = 개헤단단\n","        \"\"\" decoder mask\n","        F T T T T\n","        F F T T T\n","        F F F T T\n","        F F F T T\n","        F F F T T\n","        \"\"\"\n","        return dec_mask\n","\n","    def make_enc_dec_mask(self, src, trg):\n","\n","        enc_dec_mask = (src == pad_idx).unsqueeze(1).unsqueeze(2) # 개11단\n","        enc_dec_mask = enc_dec_mask.repeat(1, self.n_heads, trg.shape[1], 1) # 개헤단단\n","        \"\"\" src pad mask\n","        F F T T\n","        F F T T\n","        F F T T\n","        F F T T\n","        F F T T\n","        \"\"\"\n","        return enc_dec_mask\n","\n","    def forward(self, src, trg):\n","\n","        enc_mask = self.make_enc_mask(src)\n","        dec_mask = self.make_dec_mask(trg)\n","        enc_dec_mask = self.make_enc_dec_mask(src, trg)\n","\n","        enc_out, atten_encs = self.encoder(src, enc_mask)\n","        out, atten_decs, atten_enc_decs = self.decoder(trg, enc_out, dec_mask, enc_dec_mask)\n","\n","        return out, atten_encs, atten_decs, atten_enc_decs"],"metadata":{"id":"bFOSWtwqOK9s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### tril, initialization 실험"],"metadata":{"id":"bv1eQesKOmOt"}},{"cell_type":"code","source":["# 즉, Layer norm은 initialization에서 제외!\n","print(nn.LayerNorm(10).weight)\n","print(nn.LayerNorm(10).weight.dim())"],"metadata":{"id":"n-GzOgXEOn3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Kaiming vs Xavier\n","layer = nn.Linear(2,10)\n","print(layer.weight)\n","nn.init.kaiming_uniform_(layer.weight)\n","print(layer.weight) # in 만 보니까 분산 크다\n","nn.init.xavier_uniform_(layer.weight)\n","print(layer.weight) # in out 둘다 보니까 분산 작다"],"metadata":{"id":"1FyxUInEOn6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.ones(2,3,5,5)\n","x = torch.tril(x) # tril: lower triangular -> 마스크 만들 때 사용\n","print(x)"],"metadata":{"id":"6yiQ32IKOrXJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 생성"],"metadata":{"id":"S5iVW4rkOsZR"}},{"cell_type":"code","source":["model = Transformer(vocab_size, max_len, n_layers, d_model, d_ff, n_heads, drop_p).to(DEVICE)\n","\n","src = torch.tensor([[4,6,5,1,1,1],[7,7,1,1,1,1]]).to(DEVICE)\n","trg = torch.tensor([[2,5,4,4,3,1,1],[2,9,6,7,3,1,1]]).to(DEVICE)\n","\n","model.eval()\n","with torch.no_grad():\n","    x, enc_atten, dec_atten, enc_dec_atten = model(src, trg)\n","print(x.shape)\n","print(enc_atten.shape)\n","print(dec_atten.shape)\n","print(enc_dec_atten.shape)\n","print(x)"],"metadata":{"id":"n8CZm9ktOrZU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train, Test, loss_epoch 함수"],"metadata":{"id":"_09KBHuSOuJ9"}},{"cell_type":"code","source":["def Train(model, train_DL, val_DL, criterion, optimizer, scheduler = None):\n","    loss_history = {\"train\": [], \"val\": []}\n","    best_loss = 9999\n","    for ep in range(EPOCH):\n","        model.train() # train mode로 전환\n","        train_loss = loss_epoch(model, train_DL, criterion, optimizer = optimizer, scheduler = scheduler)\n","        loss_history[\"train\"] += [train_loss]\n","\n","        model.eval() # test mode로 전환\n","        with torch.no_grad():\n","            val_loss = loss_epoch(model, val_DL, criterion)\n","            loss_history[\"val\"] += [val_loss]\n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                torch.save({\"model\": model,\n","                            \"ep\": ep,\n","                            \"optimizer\": optimizer,\n","                            \"scheduler\": scheduler,}, save_model_path)\n","        # print loss\n","        print(f\"Epoch {ep+1}: train loss: {train_loss:.5f}   val loss: {val_loss:.5f}   current_LR: {optimizer.param_groups[0]['lr']:.8f}\")\n","        print(\"-\" * 20)\n","\n","    torch.save({\"loss_history\": loss_history,\n","                \"EPOCH\": EPOCH,\n","                \"BATCH_SIZE\": BATCH_SIZE}, save_history_path)\n","\n","def Test(model, test_DL, criterion):\n","    model.eval() # test mode로 전환\n","    with torch.no_grad():\n","        test_loss = loss_epoch(model, test_DL, criterion)\n","    print(f\"Test loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}\")\n","\n","def loss_epoch(model, DL, criterion, optimizer = None, scheduler = None):\n","    N = len(DL.dataset) # the number of data\n","\n","    rloss=0\n","    for src_texts, trg_texts in tqdm(DL, leave=False):\n","        src = tokenizer(src_texts, padding=True, truncation=True, max_length = max_len, return_tensors='pt').input_ids.to(DEVICE)\n","        trg_texts = ['</s> ' + s for s in trg_texts]\n","        trg = tokenizer(trg_texts, padding=True, truncation=True, max_length = max_len, return_tensors='pt').input_ids.to(DEVICE)\n","        # inference\n","        y_hat = model(src, trg[:,:-1])[0] # 모델 통과 시킬 땐 trg의 마지막 토큰은 제외!\n","        # y_hat.shape = 개단차 즉, 훈련 땐 문장이 한번에 튀어나옴\n","        # loss\n","        loss = criterion(y_hat.permute(0,2,1), trg[:,1:]) # loss 계산 시엔 <sos> 는 제외!\n","        # 개단차 -> 개차단으로 바꿔줌 (1D segmentation으로 생각)\n","        # 개채행열(예측), 개행열(정답)으로 주거나 개채1열, 개1열로 주거나 개채열, 개열로 줘야하도록 함수를 만들어놔서\n","        # 우리 상황에서는 개차단, 개단 으로 줘야 한다.\n","        # 이렇게 함수를 만들어놔야 1D, 2D segmentation 등등으로 확장가능하기 때문\n","        # 다 필요없고, 그냥 y_hat=개차단, trg=개단으로 줘야만 계산 제대로 된다고 생각하시면 됩니다!\n","        # update\n","        if optimizer is not None:\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        if scheduler is not None:\n","            scheduler.step()\n","        # loss accumulation\n","        loss_b = loss.item() * src.shape[0]\n","        rloss += loss_b\n","    loss_e = rloss/N\n","    return loss_e\n","\n","def count_params(model):\n","    num = sum([p.numel() for p in model.parameters() if p.requires_grad])\n","    return num\n","\n","class NoamScheduler:\n","    def __init__(self, optimizer, d_model, warmup_steps, LR_scale = 1):\n","        self.optimizer = optimizer\n","        self.current_step = 0\n","        self.d_model = d_model\n","        self.warmup_steps = warmup_steps\n","        self.LR_scale = LR_scale\n","\n","    def step(self):\n","        self.current_step += 1\n","        lrate = self.LR_scale * (self.d_model ** -0.5) * min(self.current_step ** -0.5, self.current_step * self.warmup_steps ** -1.5)\n","        self.optimizer.param_groups[0]['lr'] = lrate\n","\n","def plot_scheduler(scheduler_name, optimizer, scheduler, total_steps): # LR curve 보기\n","    lr_history = []\n","    steps = range(1, total_steps)\n","\n","    for _ in steps: # base model -> 10만 steps (12시간), big model -> 30만 steps (3.5일) 로 훈련했다고 함\n","        lr_history += [optimizer.param_groups[0]['lr']]\n","        scheduler.step()\n","\n","    plt.figure()\n","    if scheduler_name == 'Noam':\n","        if total_steps == 100000:\n","            plt.plot(steps, (512 ** -0.5) * torch.tensor(steps) ** -0.5, 'g--', linewidth=1, label=r\"$d_{\\mathrm{model}}^{-0.5} \\cdot \\mathrm{step}^{-0.5}$\")\n","            plt.plot(steps, (512 ** -0.5) * torch.tensor(steps) * 4000 ** -1.5, 'r--', linewidth=1, label=r\"$d_{\\mathrm{model}}^{-0.5} \\cdot \\mathrm{step} \\cdot \\mathrm{warmup\\_steps}^{-1.5}$\")\n","        plt.plot(steps, lr_history, 'b', linewidth=2, alpha=0.8, label=\"Learning Rate\")\n","    elif scheduler_name == 'Cos':\n","        plt.plot(steps, lr_history, 'b', linewidth=2, alpha=0.8, label=\"Learning Rate\")\n","    plt.ylim([-0.1*max(lr_history), 1.2*max(lr_history)])\n","    plt.xlabel('Step')\n","    plt.ylabel('Learning Rate')\n","    plt.grid()\n","    plt.legend()\n","    plt.show()"],"metadata":{"id":"Dk9jvAnoOuQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.Adam(nn.Linear(1, 1).parameters(), lr=0) # 테스트용 optimizer\n","scheduler = NoamScheduler(optimizer, d_model=512, warmup_steps=4000) # 논문 값\n","plot_scheduler(scheduler_name = 'Noam', optimizer = optimizer, scheduler = scheduler, total_steps = 100000)\n","\n","optimizer = optim.Adam(nn.Linear(1, 1).parameters(), lr=0)\n","scheduler = NoamScheduler(optimizer, d_model=d_model, warmup_steps=warmup_steps, LR_scale=LR_scale)\n","plot_scheduler(scheduler_name = 'Noam', optimizer = optimizer, scheduler = scheduler, total_steps = int(len(train_DS)*EPOCH/BATCH_SIZE)) # 내 상황\n","\n","optimizer = optim.Adam(nn.Linear(1, 1).parameters(), lr=LR_init)\n","scheduler = CosineAnnealingWarmRestarts(optimizer, T0, T_mult)\n","plot_scheduler(scheduler_name = 'Cos', optimizer = optimizer, scheduler = scheduler, total_steps = int(len(train_DS)*EPOCH/BATCH_SIZE))"],"metadata":{"id":"AMKSl3aEOzf5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 학습"],"metadata":{"id":"TtR0Rja9O0yA"}},{"cell_type":"code","source":["if new_model_train:\n","    params = [p for p in model.parameters() if p.requires_grad] # 사전 학습된 layer를 사용할 경우\n","    if scheduler_name == 'Noam':\n","        optimizer = optim.Adam(params, lr=0,\n","                               betas=(0.9, 0.98), eps=1e-9,\n","                               weight_decay=LAMBDA) # 논문에서 제시한 beta와 eps 사용, l2-Regularization은 한번 써봄 & 맨 처음 step 의 LR=0으로 출발 (warm-up)\n","        scheduler = NoamScheduler(optimizer, d_model=d_model, warmup_steps=warmup_steps, LR_scale=LR_scale)\n","\n","    elif scheduler_name == 'Cos':\n","        optimizer = optim.Adam(params, lr=LR_init,  # cos restart sheduling\n","                               betas=(0.9, 0.98), eps=1e-9,\n","                               weight_decay=LAMBDA)\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T0, T_mult)\n","\n","    Train(model, train_DL, val_DL, criterion, optimizer, scheduler)"],"metadata":{"id":"-9oVUVw_O05O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 로드 모델"],"metadata":{"id":"ivFjOev3O3NN"}},{"cell_type":"code","source":["loaded = torch.load(save_model_path, map_location=DEVICE)\n","load_model = loaded[\"model\"]\n","ep = loaded[\"ep\"]\n","optimizer = loaded[\"optimizer\"]\n","\n","loaded = torch.load(save_history_path, map_location=DEVICE)\n","loss_history = loaded[\"loss_history\"]\n","\n","print(ep)\n","print(optimizer)"],"metadata":{"id":"OWyWW9tvO4Ud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure()\n","plt.plot(range(1,EPOCH+1),loss_history[\"train\"], label=\"train\")\n","plt.plot(range(1,EPOCH+1),loss_history[\"val\"], label=\"val\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Train, Val Loss\")\n","plt.grid()\n","plt.legend()"],"metadata":{"id":"XyMjOUDYO5y2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Test(load_model, test_DL, criterion)\n","count_params(load_model)"],"metadata":{"id":"kuaJM3DpO7G7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perplxity 구하기\n","y_hat = torch.tensor([[[0.3659, 0.7025, 0.3104]], [[0.0097, 0.6577, 0.1947]],[[0.5659, 0.0025, 0.0104]], [[0.9097, 0.0577, 0.7947]]])\n","target = torch.tensor([[2],  [1], [2],  [1]])\n","\n","soft = nn.Softmax(dim=-1)\n","y_hat_soft = soft(y_hat)\n","print(y_hat_soft.shape)\n","v=1\n","for i, val in enumerate(y_hat_soft):\n","    v*=val[0,target[i]]\n","print(v**(-1/target.shape[0]))\n","# 3.5257\n","\n","criterion_test = nn.CrossEntropyLoss()\n","print(y_hat.permute(0,2,1).shape)\n","print(target.shape)\n","print(torch.exp(criterion_test(y_hat.permute(0,2,1), target))) # 결론: loss에 torch.exp 취하셈\n","# 3.5257"],"metadata":{"id":"3q5En1YoO7Jj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 번역 함수, 어텐션 map 그리는 함수"],"metadata":{"id":"d_t9TZbvTUCd"}},{"cell_type":"code","source":["def translation(model, src_text, atten_map_save = False):\n","    model.eval()\n","    with torch.no_grad():\n","        src = tokenizer.encode(src_text, return_tensors='pt').to(DEVICE) # 1x단\n","        enc_mask = model.make_enc_mask(src)\n","        enc_out, atten_enc = model.encoder(src, enc_mask, atten_map_save)\n","\n","        pred = tokenizer.encode('</s>', return_tensors='pt', add_special_tokens=False).to(DEVICE) # 1x1\n","        for _ in range(max_len):\n","            dec_mask = model.make_dec_mask(pred)\n","            enc_dec_mask = model.make_enc_dec_mask(src, pred)\n","            out, atten_dec, atten_enc_dec = model.decoder(pred, enc_out, dec_mask, enc_dec_mask, atten_map_save)\n","\n","            pred_word = out.argmax(dim=2)[:,-1].unsqueeze(0) # shape = (1,1)\n","            pred = torch.cat([pred, pred_word], dim=1) # 1x단 (단은 하나씩 늘면서)\n","\n","            if tokenizer.decode(pred_word.item()) == '</s>':\n","                break\n","\n","        translated_text = tokenizer.decode(pred[0])\n","\n","    return translated_text, atten_enc, atten_dec, atten_enc_dec\n","\n","def show_attention(atten, Query, Key, n):\n","    atten = atten.cpu()\n","\n","    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=[atten.shape[3]*1.5,atten.shape[2]])\n","    for i in range(3):\n","        ax[i].set_yticks(range(atten.shape[2]))\n","        ax[i].set_yticklabels(Query, rotation=45)\n","        ax[i].set_xticks(range(atten.shape[3]))\n","        ax[i].set_xticklabels(Key, rotation=60)\n","        ax[i].imshow(atten[n][i], cmap='bone') # n 번째 layer, 앞 세 개의 헤드만 plot\n","        # ax[i].xaxis.tick_top()  # x축 레이블을 위쪽으로 이동"],"metadata":{"id":"W3FNGQ_JTVN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 번역해보기\n","i = 7\n","idx = test_DS.indices[i]\n","src_text, trg_text = custom_DS.__getitem__(idx)\n","print(f\"입력: {src_text}\")\n","print(f\"정답: {trg_text}\")\n","\n","translated_text, atten_enc, atten_dec, atten_enc_dec = translation(load_model, src_text, atten_map_save = True)\n","print(f\"AI의 번역: {translated_text}\")"],"metadata":{"id":"fK7h_WopTVQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # teacher forcing 으로 넣어본다면?\n","\n","# src = tokenizer.encode(src_text, return_tensors='pt').to(DEVICE)\n","# trg_text = '</s> ' + trg_text\n","# trg = tokenizer.encode(trg_text, return_tensors='pt').to(DEVICE)\n","\n","# load_model.eval()\n","# with torch.no_grad():\n","#     y_hat = load_model(src, trg[:,:-1])[0]\n","#     loss = criterion(y_hat.permute(0,2,1), trg[:,1:])\n","#     out_token = y_hat.argmax(-1).squeeze()\n","#     out_text = tokenizer.decode(out_token)\n","\n","# print('loss =', loss)\n","# print('정답 =', [tokenizer.decode(i) for i in trg[0,1:]])\n","# print('예측 =', [tokenizer.decode(i) for i in out_token])"],"metadata":{"id":"eoXJpzdxTVWR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc_input = tokenizer.tokenize(src_text+' </s>') # <eos> 붙여서 학습 시켰기 때문에 여기도 붙여줘야\n","dec_tokens = tokenizer.tokenize(translated_text)\n","dec_input = dec_tokens[:-1] # 디코더 입력으로 들어가는 문장(sos 는 있고 eos는 없고)\n","dec_output = dec_tokens[1:] # 디코더 출력으로 나간 문장\n","\n","# print(dec_input)\n","# print(atten_enc_dec.shape)\n","# print(len(dec_input))\n","# print(len(enc_input))\n","\n","show_attention(atten_enc, enc_input, enc_input, n = 2) # 백화점 1층은 사원 헤드 (두루두루) 2층은 부장 헤드 3층은 임원 헤드\n","show_attention(atten_dec, dec_input, dec_input, n = 2)\n","show_attention(atten_enc_dec, dec_output, enc_input, n = 2) # 이 map을 해석할 때는 \"이 단어가 나오게끔 뭘 주목했느냐\" 로 해석해줘야 함 (ytick에 들어가는 단어가 아닌 예측한 단어를 썼기 때문)"],"metadata":{"id":"CV0uccJ1TVY9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BLEU score 구하기"],"metadata":{"id":"y5m_XN5oTas_"}},{"cell_type":"code","source":["from torchtext.data.metrics import bleu_score\n","\n","# trgs = [[['훌륭한', '강사와', '훌륭한', '수강생이','만나면','명강의가', '탄생한다']]]\n","# preds = [['훌륭한', '강사와', '훌륭한', '수강생이','함께라면','명강의가','만들어진다']]\n","# preds = [['만들어진다', '강사와', '훌륭한', '명강의가','훌륭한','수강생이','함께라면']]\n","# preds = [['훌륭한', '강사와', '훌륭한', '수강생이','훌륭한','강사와','훌륭한','강의를', '만든다']]\n","# preds = [['수강생이', '만나면', '명강의가', '탄생한다']]\n","\n","trgs = [[['훌륭한', '강사와', '훌륭한', '수강생이','만나면','명강의가', '탄생한다']], [['이것은', '두','번째','문장입니다']]]\n","preds = [['훌륭한', '강사와', '훌륭한', '수강생이','훌륭한','강의를','만든다'], ['이것은','문장입니다']]\n","\n","bleu_score(preds, trgs, max_n = 4, weights = [0.25,0.25,0.25,0.25]) # default\n","# bleu_score(preds, trgs, max_n = 1, weights = [1])"],"metadata":{"id":"qgd5DNWxTVcP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_bleu_score(model, DS):\n","    trgs = []\n","    preds = []\n","\n","    for i, (src_text, trg_text) in enumerate(DS):\n","\n","        translated_text, _, _, _ = translation(load_model, src_text)\n","\n","        trg = tokenizer.tokenize(trg_text)\n","        translated_tok = tokenizer.tokenize(translated_text)[1:-1] # <sos> & <eos> 제외\n","\n","        trgs += [[trg]]\n","        preds += [translated_tok]\n","\n","        if (i + 1) % 100 == 0:\n","            print(f\"[{i + 1}/{len(DS)}]\")\n","            print(f\"입력: {src_text}\")\n","            print(f\"정답: {trg_text}\")\n","            print(f\"AI의 번역: {translated_text[5:-4]}\") # 문자열에서 </s> 안보이게 하려고..\n","\n","    bleu = bleu_score(preds, trgs)\n","    print()\n","    print(f'Total BLEU Score = {bleu*100:.2f}')"],"metadata":{"id":"TbnF-WQkTcJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_bleu_score(load_model, test_DS)"],"metadata":{"id":"pT7qowq4TcL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 내 번역기 써보기!\n","src_text = \"안녕하세요! 이 강의 정말 열심히 준비 했어요.\"\n","print(f\"입력: {src_text}\")\n","\n","translated_text, _, _, _ = translation(load_model, src_text)\n","print(f\"AI의 번역: {translated_text}\")"],"metadata":{"id":"XQlILpnGVDKc"},"execution_count":null,"outputs":[]}]}