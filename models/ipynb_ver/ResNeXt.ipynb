{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPnmmcllA0bFarEh8NpNQvY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Zl19Nx4J4TWb"},"outputs":[],"source":["import torch\n","from torch import nn"]},{"cell_type":"code","source":["class Bottleneck(nn.Module):\n","    expansion = 2 # 클래스 속성\n","    def __init__(self, in_channels, inner_channels, cardinality, stride = 1, projection = None):\n","        super().__init__()\n","\n","        self.residual = nn.Sequential(nn.Conv2d(in_channels, inner_channels, 1, bias=False),\n","                                      nn.BatchNorm2d(inner_channels),\n","                                      nn.ReLU(inplace=True),\n","                                      nn.Conv2d(inner_channels, inner_channels, 3, stride=stride, padding=1, groups = cardinality, bias=False),\n","                                      nn.BatchNorm2d(inner_channels),\n","                                      nn.ReLU(inplace=True),\n","                                      nn.Conv2d(inner_channels, inner_channels * self.expansion, 1, bias=False),\n","                                      nn.BatchNorm2d(inner_channels * self.expansion))\n","        self.projection = projection\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","\n","        residual = self.residual(x)\n","\n","        if self.projection is not None:\n","            shortcut = self.projection(x)\n","        else:\n","            shortcut = x\n","\n","        out = self.relu(residual + shortcut)\n","        return out\n","\n","class ResNeXt(nn.Module):\n","    def __init__(self, block, num_block_list, cardinality, num_classes = 1000, zero_init_residual = True):\n","        super().__init__()\n","\n","        self.in_channels = 64\n","        self.cardinality = cardinality\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True) # 좀더 메모리 효율적\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.stage1 = self.make_stage(block, 128, num_block_list[0], stride=1)\n","        self.stage2 = self.make_stage(block, 256, num_block_list[1], stride=2)\n","        self.stage3 = self.make_stage(block, 512, num_block_list[2], stride=2)\n","        self.stage4 = self.make_stage(block, 1024, num_block_list[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(1024 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, block):\n","                    nn.init.constant_(m.residual[-1].weight, 0)\n","\n","    def make_stage(self, block, inner_channels, num_blocks, stride = 1):\n","\n","        if stride != 1 or self.in_channels != inner_channels * block.expansion:\n","            projection = nn.Sequential(\n","                nn.Conv2d(self.in_channels, inner_channels * block.expansion, 1, stride=stride, bias=False),\n","                nn.BatchNorm2d(inner_channels * block.expansion))\n","        else:\n","            projection = None\n","\n","        layers = []\n","        layers += [block(self.in_channels, inner_channels, self.cardinality, stride, projection)] # projection은 첫 block에서만\n","        self.in_channels = inner_channels * block.expansion\n","        for _ in range(1, num_blocks):\n","            layers += [block(self.in_channels, inner_channels, self.cardinality)]\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.stage1(x)\n","        x = self.stage2(x)\n","        x = self.stage3(x)\n","        x = self.stage4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        return x"],"metadata":{"id":"0UxwnbYW4X4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def resnext50(**kwargs):\n","    return ResNeXt(Bottleneck, [3, 4, 6, 3], cardinality=32, **kwargs)\n","\n","def resnext101(**kwargs):\n","    return ResNeXt(Bottleneck, [3, 4, 23, 3], cardinality=32, **kwargs)"],"metadata":{"id":"Ywe5JZL_4ZUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = resnext50()\n","# print(model)\n","!pip install torchinfo\n","from torchinfo import summary\n","summary(model, input_size=(2,3,224,224), device='cpu')"],"metadata":{"id":"mbl_jHFL4ZXj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(2,3,224,224)\n","print(model(x).shape)"],"metadata":{"id":"GsIV-HwE4bwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(nn.Conv2d(128, 256, 3, groups=1).weight.shape)\n","print(nn.Conv2d(128, 256, 3, groups=32).weight.shape)\n","# 입력 들어온 128을 32그룹으로 쪼개서 봄 -> 그래서 각 그룹이 한 채널을 만들면 128 -> 32가 됨\n","# 따라서, 32의 배수가 돼야 함. 128 -> 256 이면 weight shape은 32*8, 4, 3, 3 으로 concat 해놓음\n","# 그냥 conv에 비해 weight 개수가 32배 줄어드는 효과!"],"metadata":{"id":"6HWdTkVc4dax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(nn.Conv2d(128, 50, 3, groups=32).weight.shape) # ValueError: out_channels must be divisible by groups"],"metadata":{"id":"c5lF_Sy44ddf"},"execution_count":null,"outputs":[]}]}