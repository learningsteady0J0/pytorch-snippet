{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrgsgcCcnvuUXiQyuzGZ+Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gA4ItfLtqiCN"},"outputs":[],"source":["# Full pre-activation 사용"]},{"cell_type":"code","source":["import torch\n","from torch import nn"],"metadata":{"id":"3IboySyUwAjz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class WiderBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, projection=None, drop_p=0.3):\n","        # drop_p = 0.3 for CIFAR, 0.4 for SVHN\n","        super().__init__()\n","\n","        self.residual = nn.Sequential(nn.BatchNorm2d(in_channels),\n","                                      nn.ReLU(inplace=True),\n","                                      nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias = False),\n","                                      nn.BatchNorm2d(out_channels),\n","                                      nn.ReLU(inplace=True),\n","                                      nn.Dropout(drop_p),\n","                                      nn.Conv2d(out_channels, out_channels, 3, padding=1, bias = False))\n","\n","        self.projection = projection\n","\n","    def forward(self, x):\n","\n","        residual = self.residual(x)\n","\n","        if self.projection is not None:\n","            shortcut = self.projection(x)\n","        else:\n","            shortcut = x\n","\n","        out = residual + shortcut # 엉! ReLU 였는데 ReLU 없음!\n","        return out\n","\n","class WRN(nn.Module):\n","    def __init__(self, depth, k, num_classes=1000, init_weights=True):\n","        super().__init__()\n","        N = int((depth-4)/3/2)\n","        # 4가 아닌 2를 빼는게 맞아보이긴 하는데,, 논문에서 말한 40층이 되려면 N=6에 대해 6*2*3+\"4\" 여야 40이 맞아서.. 추측컨데 projection 하는 conv도 센거 같다\n","        self.in_channels = 16\n","\n","        self.conv1 = nn.Conv2d(3, 16, 3, padding=1, bias = False)\n","        # pre-act 구조에선 첫번째 conv block에서 pool 있으면 conv-BN-relu-pool -> Bottleneck 이렇게\n","        # 이유는? 맨처음에 bn-relu를 통과시키면 데이터 전처리에서 할 일을 하게 되는 셈이다\n","        # 근데 WRN 처럼 Block 들어가기 전 pooling이 없으면? conv -> Block 으로 바로 들어가는 듯 why?\n","        # conv-bn-relu -> Block 으로 넣으면 Block 에서 bn-relu를 만나서 bn-relu-bn-relu 이렇게 돼버린다!\n","        self.stage1 = self.make_stage(16*k, N, stride = 1)\n","        self.stage2 = self.make_stage(32*k, N, stride = 2)\n","        self.stage3 = self.make_stage(64*k, N, stride = 2)\n","        self.bn = nn.BatchNorm2d(64*k)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n","        self.fc = nn.Linear(64*k, num_classes)\n","\n","        # weight initialization\n","        if init_weights:\n","            for m in self.modules():\n","                if isinstance(m, nn.Conv2d):\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                    if m.bias is not None:\n","                        nn.init.constant_(m.bias, 0)\n","                elif isinstance(m, nn.Linear):\n","                    nn.init.normal_(m.weight, 0, 0.01)\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.stage1(x)\n","        x = self.stage2(x)\n","        x = self.stage3(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        x = self.avg_pool(x)\n","        x = torch.flatten(x, start_dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","    def make_stage(self, out_channels, num_blocks, stride):\n","\n","        if stride != 1 or self.in_channels != out_channels:\n","            projection = nn.Conv2d(self.in_channels, out_channels, 1, stride=stride, bias = False)\n","                # nn.BatchNorm2d(inner_channels * block.expansion)) # pre-act 라서 여기선 생략\n","        else:\n","            projection = None\n","\n","        layers = []\n","        layers += [WiderBlock(self.in_channels, out_channels, stride, projection)] # projection은 첫 block에서만\n","        self.in_channels = out_channels\n","        for _ in range(1, num_blocks):\n","            layers += [WiderBlock(self.in_channels, out_channels)]\n","\n","        return nn.Sequential(*layers)"],"metadata":{"id":"Kf48qhm7wBP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = WRN(depth=28, k=10, num_classes=10)\n","# print(model)\n","!pip install torchinfo\n","from torchinfo import summary\n","summary(model, (2,3, 224, 224), device=\"cpu\")"],"metadata":{"id":"-wXVxN39wEyv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(2,3,224,224)\n","print(model(x).shape)"],"metadata":{"id":"Ca-ZwHrewE2O"},"execution_count":null,"outputs":[]}]}