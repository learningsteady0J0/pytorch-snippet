{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMZhXxAC3rcdbX9+zz6n8I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yuFrjy0qOnO9"},"outputs":[],"source":[" import torch\n","from torch import nn"]},{"cell_type":"code","source":["class DepSepConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride):\n","        super().__init__()\n","\n","        self.depthwise = nn.Sequential(nn.Conv2d(in_channels,in_channels,3, stride = stride, padding = 1, groups = in_channels, bias=False),\n","                                       nn.BatchNorm2d(in_channels),\n","                                       nn.ReLU6(inplace=True))\n","\n","        self.pointwise = nn.Sequential(nn.Conv2d(in_channels,out_channels,1, bias=False),\n","                                       nn.BatchNorm2d(out_channels))\n","                                       # no activation!!\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","class InvertedBlock(nn.Module):\n","    def __init__(self, in_channels, exp_channels, out_channels, stride):\n","        super().__init__()\n","\n","        self.use_skip_connect = (stride==1 and in_channels==out_channels)\n","\n","        layers = []\n","        if in_channels != exp_channels: # 채널 안늘어날 때는 1x1 생략. 즉, 1x1은 채널을 키워야할 때만 존재한다.\n","            layers += [nn.Sequential(nn.Conv2d(in_channels, exp_channels, 1, bias=False),\n","                                     nn.BatchNorm2d(exp_channels),\n","                                     nn.ReLU6(inplace=True))]\n","        layers += [DepSepConv(exp_channels, out_channels, stride=stride)]\n","\n","        self.residual = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        if self.use_skip_connect:\n","            return x + self.residual(x) # 더하고 ReLU 하지 않는다! 그래야 linear block이 되는 거니까\n","        else:\n","            return self.residual(x)\n","\n","class MobileNetV2(nn.Module):\n","    def __init__(self, num_classes=1000):\n","        super().__init__()\n","\n","        self.configs=[# t, c, n, s\n","                      [1, 16, 1, 1],\n","                      [6, 24, 2, 2],\n","                      [6, 32, 3, 2],\n","                      [6, 64, 4, 2],\n","                      [6, 96, 3, 1],\n","                      [6, 160, 3, 2],\n","                      [6, 320, 1, 1]]\n","\n","        self.stem_conv = nn.Sequential(nn.Conv2d(3, 32, 3, padding=1, stride=2, bias=False),\n","                                       nn.BatchNorm2d(32),\n","                                       nn.ReLU6(inplace=True))\n","\n","        in_channels = 32\n","        layers = []\n","        for t, c, n, s in self.configs:\n","            for i in range(n):\n","                stride = s if i == 0 else 1\n","                exp_channels = in_channels * t\n","                layers += [InvertedBlock(in_channels=in_channels, exp_channels=exp_channels, out_channels=c, stride=stride)]\n","                in_channels = c\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","        self.last_conv = nn.Sequential(nn.Conv2d(in_channels, 1280, 1, bias=False),\n","                                       nn.BatchNorm2d(1280),\n","                                       nn.ReLU6(inplace=True))\n","\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n","\n","        self.classifier = nn.Sequential(nn.Dropout(0.2), # 논문에는 상세히 나와있진 않지만 토치 문서에 있어서 포함 -> 채널 축으로 특징들이 놓여있고 그것들을 일부 가려보며 학습하는 의미\n","                                        nn.Linear(1280, num_classes))\n","\n","    def forward(self, x):\n","        x = self.stem_conv(x)\n","        x = self.layers(x)\n","        x = self.last_conv(x)\n","        x = self.avg_pool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"Ave0OFZEO42a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = MobileNetV2()\n","# print(model)\n","!pip install torchinfo\n","from torchinfo import summary\n","summary(model, input_size=(2,3,224,224), device='cpu')"],"metadata":{"id":"xyt9cOg4O441"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(2,3,224,224)\n","print(model(x).shape)"],"metadata":{"id":"3mGiHa9YO47b"},"execution_count":null,"outputs":[]}]}