{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDezl/hkn9kdTdHzurGdNn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQqD73dBssyp"},"outputs":[],"source":["import torch\n","from torch import nn"]},{"cell_type":"code","source":["def _make_divisible(v, divisor, min_value=None):\n","    \"\"\"\n","    This function is taken from the original tf repo.\n","    It ensures that all layers have a channel number that is divisible by 8\n","    It can be seen here:\n","    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n","    :param v:\n","    :param divisor:\n","    :param min_value:\n","    :return:\n","    \"\"\"\n","    # 쉽게 말해, 이 함수는 가까운 8의 배수를 찾아줌\n","\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor) # divisor / 2 는 반올림을 위해 (너무 작아지지 않게)\n","    # case 1) v=10, divisor = 8 이면 10+4 // 8 * 8 = 8 근데 10 => 8 은 10% 이상 빠지는 거니까 8+8 = 16 으로 조정됨\n","    # case 2) v=39, divisor = 8 이면 39+4 // 8 * 8 = 40 => 10%보다 빠지지 않았기 때문에 40이 출력됨!\n","\n","    if new_v < 0.9 * v: # 10% 보다 더 빠지지 않게 조정\n","        new_v += divisor\n","\n","    return new_v\n","\n","class SEBlock(nn.Module):\n","    def __init__(self, in_channels, r = 4): # mobilenet V3 에서는 reduction ratio r=4로!\n","        super().__init__()\n","        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n","        self.excitation = nn.Sequential(nn.Linear(in_channels, _make_divisible(in_channels // r, 8)),\n","                                        nn.ReLU(inplace=True),\n","                                        nn.Linear(_make_divisible(in_channels // r, 8), in_channels),\n","                                        nn.Hardsigmoid(inplace=True)) # Hard sigmoid!\n","\n","    def forward(self, x):\n","        SE = self.squeeze(x)\n","        SE = SE.reshape(x.shape[0],x.shape[1])\n","        SE = self.excitation(SE)\n","        SE = SE.unsqueeze(dim=2).unsqueeze(dim=3)\n","        x = x * SE\n","        return x\n","\n","class DepSESep(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, use_se, use_hs, stride):\n","        super().__init__()\n","\n","        self.depthwise = nn.Sequential(nn.Conv2d(in_channels, in_channels, kernel_size, stride = stride, padding = (kernel_size - 1) // 2, groups = in_channels, bias=False),\n","                                       nn.BatchNorm2d(in_channels, momentum=0.99), # momentum = 0.99 는 논문에서 제시\n","                                       nn.Hardswish(inplace=True) if use_hs else nn.ReLU(inplace=True)) # hs 아니면 걍 ReLU (ReLU6 아님)\n","\n","        self.seblock = SEBlock(in_channels) if use_se else None\n","\n","        self.pointwise = nn.Sequential(nn.Conv2d(in_channels, out_channels,1, bias=False),\n","                                       nn.BatchNorm2d(out_channels, momentum=0.99))\n","                                       # no activation!!\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        if self.seblock is not None:\n","            x = self.seblock(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","class InvertedBlock(nn.Module):\n","    def __init__(self, in_channels, exp_channels, out_channels, kernel_size, stride, use_se, use_hs):\n","        super().__init__()\n","\n","        self.use_skip_connect = (stride==1 and in_channels==out_channels)\n","\n","        layers = []\n","        if in_channels != exp_channels: # 채널 안늘어날 때는 1x1 생략. 즉, 1x1은 채널을 키워야할 때만 존재한다.\n","            layers += [nn.Sequential(nn.Conv2d(in_channels, exp_channels, 1, bias=False),\n","                                     nn.BatchNorm2d(exp_channels, momentum=0.99),\n","                                     nn.Hardswish(inplace=True) if use_hs else nn.ReLU(inplace=True))]\n","        layers += [DepSESep(exp_channels, out_channels, kernel_size, use_se, use_hs, stride=stride)]\n","\n","        self.residual = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        if self.use_skip_connect:\n","            return x + self.residual(x) # 더하고 ReLU 하지 않는다! 그래야 linear block이 되는 거니까\n","        else:\n","            return self.residual(x)\n","\n","class MobileNetV3(nn.Module):\n","    def __init__(self, cfgs, last_channels, num_classes=1000, width_mult=1.):\n","        super().__init__()\n","\n","        in_channels = _make_divisible(16 * width_mult, 8)\n","\n","        # building first layer\n","        self.stem_conv = nn.Sequential(nn.Conv2d(3, in_channels, 3, padding=1, stride=2, bias=False),\n","                                       nn.BatchNorm2d(in_channels, momentum=0.99),\n","                                       nn.Hardswish(inplace=True)) # 처음건 무조건 HS, HS를 써서 16으로 줄일 수 있었다 함\n","\n","        # building inverted residual blocks\n","        layers=[]\n","        for k, t, c, use_se, use_hs, s in cfgs:\n","            exp_channels = _make_divisible(in_channels * t, 8)\n","            out_channels = _make_divisible(c * width_mult, 8)\n","            layers += [InvertedBlock(in_channels, exp_channels, out_channels, k, s, use_se, use_hs)]\n","            in_channels = out_channels\n","        self.layers = nn.Sequential(*layers)\n","\n","        # building last several layers\n","        self.last_conv = nn.Sequential(nn.Conv2d(in_channels, exp_channels, 1, bias=False),\n","                                       nn.BatchNorm2d(exp_channels, momentum=0.99),\n","                                       nn.Hardswish(inplace=True))\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        last_channels = _make_divisible(last_channels * width_mult, 8)\n","        self.classifier = nn.Sequential(nn.Linear(exp_channels, last_channels),\n","                                        nn.Hardswish(inplace=True),\n","                                        nn.Dropout(p=0.2, inplace=True),\n","                                        nn.Linear(last_channels, num_classes)) # MLP 부활\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.zeros_(m.bias)\n","\n","    def forward(self, x):\n","        x = self.stem_conv(x)\n","        x = self.layers(x)\n","        x = self.last_conv(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","\n","def mobilenetv3_large(**kwargs):\n","    cfgs = [#k,   t,   c,   SE,   HS,   s\n","            # 이전 output에 t를 곱해서 exp size가 되는 것임!\n","            [3,   1,  16, False, False, 1],\n","            [3,   4,  24, False, False, 2],\n","            [3,   3,  24, False, False, 1],\n","            [5,   3,  40, True,  False, 2],\n","            [5,   3,  40, True,  False, 1],\n","            [5,   3,  40, True,  False, 1],\n","            [3,   6,  80, False, True,  2],\n","            [3, 2.5,  80, False, True,  1],\n","            [3, 2.3,  80, False, True,  1],\n","            [3, 2.3,  80, False, True,  1],\n","            [3,   6, 112, True,  True,  1],\n","            [3,   6, 112, True,  True,  1],\n","            [5,   6, 160, True,  True,  2],\n","            [5,   6, 160, True,  True,  1],\n","            [5,   6, 160, True,  True,  1]]\n","\n","    return MobileNetV3(cfgs, last_channels=1280, **kwargs)\n","\n","def mobilenetv3_small(**kwargs):\n","    cfgs = [#k,    t,   c,  SE,    HS,   s\n","            [3,    1,  16, True,  False, 2],\n","            [3,  4.5,  24, False, False, 2],\n","            [3, 3.67,  24, False, False, 1],\n","            [5,    4,  40, True,  True,  2],\n","            [5,    6,  40, True,  True,  1],\n","            [5,    6,  40, True,  True,  1],\n","            [5,    3,  48, True,  True,  1],\n","            [5,    3,  48, True,  True,  1],\n","            [5,    6,  96, True,  True,  2],\n","            [5,    6,  96, True,  True,  1],\n","            [5,    6,  96, True,  True,  1]]\n","\n","    return MobileNetV3(cfgs, last_channels=1024, **kwargs)"],"metadata":{"id":"MWGvNsOlswQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = mobilenetv3_large()\n","# print(model)\n","!pip install torchinfo\n","from torchinfo import summary\n","summary(model, input_size=(2,3,224,224), device='cpu')"],"metadata":{"id":"Jc_dDjc0swSN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.randn(2,3,224,224)\n","print(model(x).shape)"],"metadata":{"id":"zaQoHNmcswUT"},"execution_count":null,"outputs":[]}]}