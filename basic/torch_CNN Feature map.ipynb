{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9KW5y2RA+AEG6aEmx8C1h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pHitVB7m8WRh"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torchvision import datasets,transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(DEVICE)"]},{"cell_type":"code","source":["BATCH_SIZE = 1\n","save_model_path = \"/content/drive/MyDrive/Colab Notebooks/results/CNN_deep_CIFAR10.pt\"\n","transform = transforms.ToTensor()\n","\n","test_DS = datasets.CIFAR10(root = '/content/drive/MyDrive/Colab Notebooks/data', train=False, download=True, transform=transforms.ToTensor())\n","test_DL = torch.utils.data.DataLoader(test_DS, batch_size=BATCH_SIZE, shuffle=True)"],"metadata":{"id":"cYMDZxJv8gie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNN_deep(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv_block1 = nn.Sequential(nn.Conv2d(3,32,3,padding=1),\n","                                         nn.BatchNorm2d(32),\n","                                         nn.ReLU(),\n","                                         nn.Conv2d(32,32,3,padding=1),\n","                                         nn.BatchNorm2d(32),\n","                                         nn.ReLU())\n","        self.Maxpool1 = nn.MaxPool2d(2)\n","\n","        self.conv_block2 = nn.Sequential(nn.Conv2d(32,64,3,padding=1),\n","                                         nn.BatchNorm2d(64),\n","                                         nn.ReLU(),\n","                                         nn.Conv2d(64,64,3,padding=1),\n","                                         nn.BatchNorm2d(64),\n","                                         nn.ReLU(),\n","                                         nn.Conv2d(64,64,3,padding=1),\n","                                         nn.BatchNorm2d(64),\n","                                         nn.ReLU())\n","        self.Maxpool2 = nn.MaxPool2d(2)\n","\n","        self.conv_block3 = nn.Sequential(nn.Conv2d(64,128,3,padding=1),\n","                                         nn.BatchNorm2d(128),\n","                                         nn.ReLU(),\n","                                         nn.Conv2d(128,128,3,padding=1),\n","                                         nn.BatchNorm2d(128),\n","                                         nn.ReLU(),\n","                                         nn.Conv2d(128,128,3,padding=1),\n","                                         nn.BatchNorm2d(128),\n","                                         nn.ReLU())\n","        self.Maxpool3 = nn.MaxPool2d(2)\n","\n","        self.classifier = nn.Sequential(nn.Linear(128*4*4,512),\n","                                        nn.Linear(512,10))\n","\n","    def forward(self, x):\n","        x = self.conv_block1(x)\n","        x = self.Maxpool1(x)\n","        x = self.conv_block2(x)\n","        x = self.Maxpool2(x)\n","        x = self.conv_block3(x)\n","        x = self.Maxpool3(x)\n","        x = torch.flatten(x, start_dim=1)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"hccQlKwq8glb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["load_model=torch.load(save_model_path, map_location=DEVICE)\n","print(load_model)"],"metadata":{"id":"LmZFQqYD8gn9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["load_model.eval()\n","with torch.no_grad():\n","    x_batch, y_batch = next(iter(test_DL))\n","    x_batch = x_batch.to(DEVICE)\n","    y_batch = y_batch.to(DEVICE)\n","    y_hat = load_model(x_batch)\n","    pred = y_hat.argmax(dim=1)\n","\n","    feature_map1 = load_model.conv_block1(x_batch)\n","    feature_map2 = load_model.conv_block2(load_model.Maxpool1(feature_map1))\n","    feature_map3 = load_model.conv_block3(load_model.Maxpool2(feature_map2))\n","\n","x_batch = x_batch.cpu()\n","feature_map1 = feature_map1.cpu()\n","feature_map2 = feature_map2.cpu()\n","feature_map3 = feature_map3.cpu()\n","\n","plt.figure(figsize=(8,8))\n","plt.xticks([]); plt.yticks([])\n","plt.imshow(x_batch[0,...].permute(1,2,0))\n","\n","print(feature_map1.shape)\n","plt.figure(figsize=(32,16))\n","for idx in range(32):\n","    plt.subplot(4,8,idx+1, xticks=[], yticks=[])\n","    plt.imshow(feature_map1[0,idx,...], cmap=\"gray\")\n","\n","print(feature_map2.shape)\n","plt.figure(figsize=(16,16))\n","for idx in range(64):\n","    plt.subplot(8,8,idx+1, xticks=[], yticks=[])\n","    plt.imshow(feature_map2[0,idx,...], cmap=\"gray\")\n","\n","print(feature_map3.shape)\n","plt.figure(figsize=(16,8))\n","for idx in range(128):\n","    plt.subplot(8,16,idx+1, xticks=[], yticks=[])\n","    plt.imshow(feature_map3[0,idx,...], cmap=\"gray\")\n","\n","summed_map = feature_map3.abs().sum(dim=1) # 음의 값을 어떻게 해석?\n","plt.figure(figsize=(8,8))\n","plt.xticks([]); plt.yticks([])\n","plt.imshow(summed_map[0,...])\n","\n","plt.figure(figsize=(8,8))\n","plt.xticks([]); plt.yticks([])\n","plt.imshow(x_batch[0,...].permute(1,2,0))\n","plt.imshow(summed_map[0,...], extent=[0,32,32,0], alpha=0.4) # feature map과 원본이미지 크기가 달라서 extent를 통해 맞춰줌\n","pred_class = test_DS.classes[pred]\n","true_class = test_DS.classes[y_batch]\n","plt.title(f\"{pred_class} ({true_class})\", color=\"g\" if pred_class==true_class else \"r\")"],"metadata":{"id":"ESBQiEpG8gqF"},"execution_count":null,"outputs":[]}]}